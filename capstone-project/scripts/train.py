# -*- coding: utf-8 -*-
"""capstone-project.ipynb

Automatically generated by Colab.
"""

import os
import shutil
from sklearn.model_selection import train_test_split

from PIL import Image
import numpy as np

import torch
import torchvision.models as models
from torchvision import transforms

from torch.utils.data import Dataset

from torch.utils.data import DataLoader

import torch.optim as optim
import torch.nn as nn



source_dir = 'blood-cell-images-for-cancer-detection'
target_dir = 'blood-cell-images-for-cancer-detection-prepared'

def split_cell_dataset(source_dir, target_dir, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2, random_state=42):
    """
    Split image dataset into train, validation and test sets with class preservation.

    Args:
        source_dir: Directory with class subdirectories
        target_dir: Directory where split datasets will be created
        train_ratio: Proportion for training set
        val_ratio: Proportion for validation set
        test_ratio: Proportion for test set
        random_state: Random seed for reproducibility
    """

    # Create target directory structure
    os.makedirs(target_dir, exist_ok=True)

    # Get list of class directories
    class_names = [d for d in os.listdir(source_dir)
                   if os.path.isdir(os.path.join(source_dir, d))]

    # Create split directories for each class
    for split in ['train', 'val', 'test']:
        for class_name in class_names:
            os.makedirs(os.path.join(target_dir, split, class_name), exist_ok=True)

    # Process each class
    for class_name in class_names:
        class_path = os.path.join(source_dir, class_name)
        images = os.listdir(class_path)

        # First split: train+val vs test
        train_val, test = train_test_split(
            images,
            test_size=test_ratio,
            random_state=random_state,
            shuffle=True
        )

        # Second split: train vs val
        val_relative_ratio = val_ratio / (train_ratio + val_ratio)
        train, val = train_test_split(
            train_val,
            test_size=val_relative_ratio,
            random_state=random_state,
            shuffle=True
        )

        # Copy files to split directories
        for img in train:
            src = os.path.join(class_path, img)
            dst = os.path.join(target_dir, 'train', class_name, img)
            shutil.copy(src, dst)

        for img in val:
            src = os.path.join(class_path, img)
            dst = os.path.join(target_dir, 'val', class_name, img)
            shutil.copy(src, dst)

        for img in test:
            src = os.path.join(class_path, img)
            dst = os.path.join(target_dir, 'test', class_name, img)
            shutil.copy(src, dst)


split_cell_dataset(source_dir, target_dir, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2, random_state=42)


# Load an image
img = Image.open('blood-cell-images-for-cancer-detection-prepared/train/basophil/BA_250000.jpg')

# Resize to target size
img = img.resize((224, 224))

# Convert to numpy array
x = np.array(img)
print(x.shape)  # (224, 224, 3)


# Load pre-trained model
model = models.mobilenet_v2(weights='IMAGENET1K_V1')
model.eval()

# Preprocessing for MobileNetV2
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

img_t = preprocess(img)
batch_t = torch.unsqueeze(img_t, 0)


class BloodDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.image_paths = []
        self.labels = []
        self.classes = sorted(os.listdir(data_dir))
        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}

        for label_name in self.classes:
            label_dir = os.path.join(data_dir, label_name)
            for img_name in os.listdir(label_dir):
                self.image_paths.append(os.path.join(label_dir, img_name))
                self.labels.append(self.class_to_idx[label_name])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('RGB')
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label

input_size = 224

# ImageNet normalization values
mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]

# Training transforms WITH augmentation
train_transforms = transforms.Compose([
    transforms.RandomRotation(10),           # Rotate up to 10 degrees
    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),  # Zoom
    transforms.RandomHorizontalFlip(),       # Horizontal flip
    transforms.ToTensor(),
    transforms.Normalize(mean=mean, std=std)
])

# Validation transforms - NO augmentation, same as before
val_transforms = transforms.Compose([
    transforms.Resize((input_size, input_size)),
    transforms.ToTensor(),
    transforms.Normalize(mean=mean, std=std)
])


train_dataset = BloodDataset(
    data_dir='./blood-cell-images-for-cancer-detection-prepared/train/',
    transform=train_transforms
)

val_dataset = BloodDataset(
    data_dir='./blood-cell-images-for-cancer-detection-prepared/val/',
    transform=val_transforms
)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

optimizer = optim.Adam(model.parameters(), lr=0.01)
criterion = nn.CrossEntropyLoss()


class BloodClassifierMobileNet(nn.Module):
    # def __init__(self, size_inner=100, droprate=0.2, num_classes=5):
    # def __init__(self, droprate=0.2, num_classes=5):
    def __init__(self, num_classes=5):
        super(BloodClassifierMobileNet, self).__init__()

        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')

        for param in self.base_model.parameters():
            param.requires_grad = False

        self.base_model.classifier = nn.Identity()

        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))
        # self.inner = nn.Linear(1280, size_inner)  # New inner layer
        self.relu = nn.ReLU()
        # self.dropout = nn.Dropout(droprate)  # Add dropout
        # self.output_layer = nn.Linear(size_inner, num_classes)
        self.output_layer = nn.Linear(1280, num_classes)

    def forward(self, x):
        x = self.base_model.features(x)
        x = self.global_avg_pooling(x)
        x = torch.flatten(x, 1)
        # x = self.inner(x)
        x = self.relu(x)
        # x = self.dropout(x)  # Apply dropout
        x = self.output_layer(x)
        return x

def make_model(
        learning_rate=0.001, # best value, defined below
        # size_inner=100, # best value, defined below
        # droprate=0.5, # best value, defined below
):
    model = BloodClassifierMobileNet(
        num_classes=5,
        # size_inner=size_inner,
        # droprate=droprate,
    )
    model.to(device)
    model.eval()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    return model, optimizer

def train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device):
    best_val_accuracy = 0.0  # Initialize variable to track the best validation accuracy


    for epoch in range(num_epochs):
        # Training phase
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)

            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_loss = running_loss / len(train_loader)
        train_acc = correct / total

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)

                outputs = model(inputs)
                loss = criterion(outputs, labels)

                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

        val_loss /= len(val_loader)
        val_acc = val_correct / val_total

        print(f'Epoch {epoch+1}/{num_epochs}')
        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')
        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')


        if val_acc > best_val_accuracy:
            best_val_accuracy = val_acc
            checkpoint_path = f'mobilenet_result_{epoch+1:02d}_{val_acc:.3f}.pth'
            torch.save(model.state_dict(), checkpoint_path)
            print(f'Checkpoint saved: {checkpoint_path}')

# num_epochs = 10
# learning_rate = 0.001

# model, optimizer = make_model(
#         learning_rate=learning_rate,
#     )
# train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)

# num_epochs = 10
# learning_rates = [0.0001, 0.001, 0.01, 0.1]

# for lr in learning_rates:
#     print(f'\n=== Learning Rate: {lr} ===')
#     model, optimizer = make_model(learning_rate=lr)
#     train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)

# learning_rate=0.001 is the best and most stable value

# num_epochs = 10
# sizes_inner = [1000, 500, 100]

# for size_inner in sizes_inner:
#     print(f'\n=== Size inner: {size_inner} ===')
#     model, optimizer = make_model(
#         learning_rate=0.001,
#         size_inner=size_inner
#     )
#     train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)

# size_inner=100 is the best and most stable value

# num_epochs = 10
# droprates = [0.1, 0.2, 0.5, 0.7]

# for droprate in droprates:
#     print(f'\n=== Droprate: {droprate} ===')
#     model, optimizer = make_model(
#         learning_rate=0.001,
#         size_inner=100,
#         droprate=droprate,
#     )
#     train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)

# droprate=0.5 is the best and most stable value

num_epochs = 50
learning_rate = 0.001
# size_inner = 100
# droprate = 0.5

model, optimizer = make_model(
        learning_rate=learning_rate,
        # size_inner=size_inner,
        # droprate=droprate,
    )

train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)